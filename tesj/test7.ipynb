{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- TEST ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding(num_tokens=5, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = Tokenizer.from_file(\"concode_tokenizer.json\")\n",
    "\n",
    "# Example text\n",
    "text = \"this is a sample code\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokenized_text = tokenizer.encode(text)\n",
    "print(tokenized_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- TEST ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tokenizer_on_concode_data(train_data, vocab_size=30000):\n",
    "    # Initialize a BPE tokenizer\n",
    "    tokenizer = Tokenizer(models.BPE())\n",
    "\n",
    "    # Set the pre-tokenizer and decoder\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=True)\n",
    "    tokenizer.decoder = decoders.ByteLevel()\n",
    "\n",
    "    # Set the trainer and train the tokenizer\n",
    "    trainer = trainers.BpeTrainer(vocab_size=vocab_size, special_tokens=[\"<s>\", \"<pad>\", \"</s>\", \"<unk>\"])\n",
    "    tokenizer.train_from_iterator(all_texts(train_data), trainer)\n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "def all_texts(data):\n",
    "    for item in data:\n",
    "        nl = item.get('nl', '')\n",
    "        code = item.get('code', '')\n",
    "        yield nl\n",
    "        yield code\n",
    "\n",
    "# Load the CONCODE dataset\n",
    "def load_concode_dataset(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            example = json.loads(line.strip())\n",
    "            data.append(example)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 30000 examples from ./data1/python_train_0.jsonl\n",
      "Trained tokenizer with 30000 examples\n",
      "Tokenized 30000 examples\n",
      "Tokenized 30000 examples\n",
      "(Encoding(num_tokens=451, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]), Encoding(num_tokens=12, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]))\n",
      "(Encoding(num_tokens=419, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]), Encoding(num_tokens=13, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]))\n",
      "(Encoding(num_tokens=267, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]), Encoding(num_tokens=8, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]))\n",
      "(Encoding(num_tokens=35, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]), Encoding(num_tokens=18, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]))\n",
      "(Encoding(num_tokens=67, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]), Encoding(num_tokens=20, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]))\n",
      "Loaded tokenizer\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def tokenize_concode_dataset(data, tokenizer):\n",
    "    tokenized_data = []\n",
    "    for item in data:\n",
    "        if 'code_tokens' in item and 'docstring_tokens' in item:\n",
    "            tokenized_code = tokenizer.encode(' '.join(item['code_tokens']))\n",
    "            tokenized_docstring = tokenizer.encode(' '.join(item['docstring_tokens']))\n",
    "            tokenized_data.append((tokenized_code, tokenized_docstring))\n",
    "    print(f\"Tokenized {len(tokenized_data)} examples\")\n",
    "    return tokenized_data\n",
    "\n",
    "\n",
    "train_file = \"./data1/python_train_0.jsonl\"\n",
    "train_data = load_concode_dataset(train_file)\n",
    "\n",
    "print(f\"Loaded {len(train_data)} examples from {train_file}\")\n",
    "\n",
    "vocab_size = 30000\n",
    "tokenizer = train_tokenizer_on_concode_data(train_data, vocab_size)\n",
    "\n",
    "print(f\"Trained tokenizer with {len(train_data)} examples\")\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save(\"concode_tokenizer.json\")\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_data = tokenize_concode_dataset(train_data, tokenizer)\n",
    "print(f\"Tokenized {len(tokenized_data)} examples\")\n",
    "\n",
    "if len(tokenized_data) >= 5:\n",
    "    for i in range(5):\n",
    "        print(tokenized_data[i])\n",
    "else:\n",
    "    print(f\"tokenized_data has only {len(tokenized_data)} elements.\")\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = Tokenizer.from_file(\"concode_tokenizer.json\")\n",
    "print(\"Loaded tokenizer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dataset with 30000 examples\n",
      "Created dataloader with 938 batches\n",
      "Tokenized 30000 examples\n"
     ]
    }
   ],
   "source": [
    "# Create a dataset and dataloader for the tokenized data\n",
    "class ConcodeDataset(Dataset):\n",
    "    def __init__(self, tokenized_data):\n",
    "        self.tokenized_data = tokenized_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.tokenized_data[idx]\n",
    "\n",
    "# Create a dataset and dataloader for the tokenized data\n",
    "dataset = ConcodeDataset(tokenized_data)\n",
    "print(f\"Created dataset with {len(dataset)} examples\")\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=1)\n",
    "print(f\"Created dataloader with {len(dataloader)} batches\")\n",
    "\n",
    "\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save(\"concode_tokenizer.json\")\n",
    "\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = Tokenizer.from_file(\"concode_tokenizer.json\")\n",
    "tokenized_data = tokenize_concode_dataset(train_data, tokenizer)\n",
    "SOS_IDX = tokenizer.token_to_id('<s>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        outputs, (hidden, cell) = self.lstm(x)\n",
    "        return hidden, cell\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_size, hidden_size, num_layers):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.embedding(x)\n",
    "        outputs, (hidden, cell) = self.lstm(x, (hidden, cell))\n",
    "        predictions = self.fc(outputs.squeeze(1))\n",
    "        return predictions, hidden, cell\n",
    "\n",
    "class EncoderDecoderModel(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(EncoderDecoderModel, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        # Get the encoder outputs and hidden state\n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "\n",
    "        # Initialize the decoder input\n",
    "        decoder_input = torch.tensor([[SOS_IDX]*trg.shape[1]])\n",
    "\n",
    "        # Move everything to device\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        encoder_outputs = encoder_outputs.to(device)\n",
    "        hidden = hidden.to(device)\n",
    "        decoder_input = decoder_input.to(device)\n",
    "\n",
    "        # Initialize the decoder outputs tensor\n",
    "        decoder_outputs = torch.zeros(trg.shape[0], trg.shape[1], self.decoder.output_dim).to(device)\n",
    "\n",
    "        # Decode one step at a time\n",
    "        for t in range(trg.shape[0]):\n",
    "            output, hidden = self.decoder(decoder_input, hidden, encoder_outputs)\n",
    "            decoder_outputs[t] = output\n",
    "            decoder_input = trg[t].unsqueeze(0)\n",
    "\n",
    "        return decoder_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Seq2SeqModel class and update it with appropriate arguments for encoder and decoder\n",
    "input_size = vocab_size\n",
    "hidden_size = 256\n",
    "output_size = vocab_size\n",
    "num_layers = 1\n",
    "\n",
    "encoder = Encoder(input_size, hidden_size, num_layers)\n",
    "decoder = Decoder(output_size, hidden_size, num_layers)\n",
    "model = EncoderDecoderModel(encoder, decoder)\n",
    "\n",
    "# Train and save the model\n",
    "def train_and_save_model(model, dataloader, num_epochs=1):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    print_every = 1  # Choose a number that suits your dataset size\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for i, data in enumerate(dataloader, 0):\n",
    "            nl_tokens, code_tokens = data\n",
    "            nl_tokens = nl_tokens.to(device)\n",
    "            code_tokens = code_tokens.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(nl_tokens, code_tokens[:, :-1])\n",
    "            loss = criterion(outputs.view(-1, outputs.size(-1)), code_tokens[:, 1:].view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Print the running loss every 'print_every' iterations\n",
    "            if (i + 1) % print_every == 0:\n",
    "                print(f'Epoch {epoch + 1}, Iteration {i + 1}, Running Loss: {running_loss / (i + 1)}')\n",
    "\n",
    "        print(f'Epoch {epoch + 1}, Loss: {running_loss / len(dataloader)}')\n",
    "\n",
    "    print(\"Finished training\")\n",
    "    torch.save(model.state_dict(), 'trained_model.pth')\n",
    "\n",
    "train_and_save_model(model, dataloader, num_epochs=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
