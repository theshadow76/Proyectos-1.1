{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 4, Loss: 4.693137288093567\n",
      "Step: 8, Loss: 4.348484635353088\n",
      "Step: 12, Loss: 4.880219101905823\n",
      "Step: 16, Loss: 4.7571399211883545\n",
      "Step: 20, Loss: 4.254218816757202\n",
      "Step: 24, Loss: 4.083686947822571\n",
      "Step: 28, Loss: 4.018662750720978\n",
      "Step: 32, Loss: 3.607541024684906\n",
      "Step: 36, Loss: 3.428359806537628\n",
      "Step: 40, Loss: 4.054280579090118\n",
      "Step: 44, Loss: 3.3034571409225464\n",
      "Step: 48, Loss: 3.372847080230713\n",
      "Step: 52, Loss: 3.383326530456543\n",
      "Step: 56, Loss: 3.4874064326286316\n",
      "Step: 60, Loss: 3.0335362553596497\n",
      "Step: 64, Loss: 3.1241634488105774\n",
      "Step: 68, Loss: 2.5820778906345367\n",
      "Step: 72, Loss: 2.5536625683307648\n",
      "Step: 76, Loss: 2.668115973472595\n",
      "Step: 80, Loss: 2.4756160974502563\n",
      "Epoch: 1, Loss: 1.8110204935073853\n",
      "Step: 4, Loss: 4.987349063158035\n",
      "Step: 8, Loss: 2.8676598072052\n",
      "Step: 12, Loss: 3.0661474466323853\n",
      "Step: 16, Loss: 2.2211397290229797\n",
      "Step: 20, Loss: 3.146446645259857\n",
      "Step: 24, Loss: 3.0171623826026917\n",
      "Step: 28, Loss: 2.4846193194389343\n",
      "Step: 32, Loss: 2.274779498577118\n",
      "Step: 36, Loss: 2.922286033630371\n",
      "Step: 40, Loss: 2.0014383494853973\n",
      "Step: 44, Loss: 3.1022666096687317\n",
      "Step: 48, Loss: 2.683787524700165\n",
      "Step: 52, Loss: 2.319516122341156\n",
      "Step: 56, Loss: 2.378455877304077\n",
      "Step: 60, Loss: 2.669637084007263\n",
      "Step: 64, Loss: 2.323141425848007\n",
      "Step: 68, Loss: 2.460978865623474\n",
      "Step: 72, Loss: 2.5362076461315155\n",
      "Step: 76, Loss: 2.2354822754859924\n",
      "Step: 80, Loss: 2.11976757645607\n",
      "Epoch: 2, Loss: 2.6791889667510986\n",
      "Step: 4, Loss: 4.466652899980545\n",
      "Step: 8, Loss: 2.537436842918396\n",
      "Step: 12, Loss: 2.140188157558441\n",
      "Step: 16, Loss: 2.4965204298496246\n",
      "Step: 20, Loss: 2.092306613922119\n",
      "Step: 24, Loss: 2.2089538872241974\n",
      "Step: 28, Loss: 2.105338305234909\n",
      "Step: 32, Loss: 2.900593400001526\n",
      "Step: 36, Loss: 3.0878254175186157\n",
      "Step: 40, Loss: 1.7815459370613098\n",
      "Step: 44, Loss: 2.673545241355896\n",
      "Step: 48, Loss: 2.6044668555259705\n",
      "Step: 52, Loss: 2.594584107398987\n",
      "Step: 56, Loss: 1.879414588212967\n",
      "Step: 60, Loss: 2.271490454673767\n",
      "Step: 64, Loss: 2.27246955037117\n",
      "Step: 68, Loss: 2.3276858031749725\n",
      "Step: 72, Loss: 2.010422945022583\n",
      "Step: 76, Loss: 2.2387479841709137\n",
      "Step: 80, Loss: 1.9761618673801422\n",
      "Epoch: 3, Loss: 1.0980443954467773\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, DataCollatorForLanguageModeling\n",
    "\n",
    "class CodeDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=512):\n",
    "        self.data = [f\"<CODE> {line.strip()} <CODE>\" for line in data]  # Add special token at the beginning and end of each code snippet\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data[idx]\n",
    "        tokens = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
    "        tokens = {key: value.squeeze(0) for key, value in tokens.items()}\n",
    "        return tokens\n",
    "\n",
    "# Load data from txt file\n",
    "with open('data4.txt', 'r') as f:\n",
    "    new_data = f.readlines()\n",
    "\n",
    "# Initialize the GPT-2 tokenizer and dataset\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('trained_4_model')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "max_length = 128  # Reduced max_length\n",
    "dataset = CodeDataset(new_data, tokenizer, max_length=max_length)\n",
    "\n",
    "# Create a DataLoader for batching\n",
    "batch_size = 2  # Reduced batch_size\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=data_collator)\n",
    "\n",
    "# Load the pre-trained GPT-2 model and set it up for training\n",
    "model = GPT2LMHeadModel.from_pretrained('trained_4_model')\n",
    "\n",
    "# Check if multiple GPUs are available and wrap the model with nn.DataParallel\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Using\", torch.cuda.device_count(), \"GPUs\")\n",
    "    model = torch.nn.DataParallel(model)\n",
    "\n",
    "model.train()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Set up the optimizer and training loop\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "epochs = 3\n",
    "gradient_accumulation_steps = 4  # Gradient accumulation\n",
    "accumulated_loss = 0.0\n",
    "for epoch in range(epochs):\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient accumulation\n",
    "        accumulated_loss += loss.item()\n",
    "        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            print(f\"Step: {step + 1}, Loss: {accumulated_loss / gradient_accumulation_steps}\")\n",
    "            accumulated_loss = 0.0\n",
    "\n",
    "    print(f\"Epoch: {epoch + 1}, Loss: {loss.item()}\")\n",
    "\n",
    "# Save the trained model\n",
    "model.save_pretrained(\"trained_5_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a Python function that calculates the sum of two numbers in a list:\n",
      "\n",
      "I know that this is a very complex way to sum the number of possible numbers using a number.\n",
      "Here is an example:\n",
      "\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50\n",
      "There is really a lot of interesting code:<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Load the original GPT-2 tokenizer\n",
    "original_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Save the tokenizer files to the \"trained_gpt2_model\" folder\n",
    "original_tokenizer.save_pretrained(\"trained_7_model\")\n",
    "\n",
    "# Load the saved model\n",
    "model = GPT2LMHeadModel.from_pretrained(\"trained_7_model\")\n",
    "model.eval()\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"trained_7_model\")\n",
    "\n",
    "# Function to generate code based on a prompt\n",
    "def generate_code(prompt, max_length=None, num_return_sequences=3, min_length=50):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    attention_mask = torch.ones(input_ids.shape, dtype=torch.long, device=input_ids.device)\n",
    "    generated_sequences = []\n",
    "    \n",
    "    for i in range(num_return_sequences):\n",
    "        output_sequences = []\n",
    "        while True:\n",
    "            output_sequence = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=max_length,\n",
    "                no_repeat_ngram_size=2,\n",
    "                do_sample=True,\n",
    "                top_k=50, # You can experiment with this value\n",
    "                top_p=0.95,\n",
    "                temperature=1.0, # You can experiment with this value\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "            generated_sequence = output_sequence[0].tolist()\n",
    "            text = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)\n",
    "            \n",
    "            if len(text) >= min_length:\n",
    "                output_sequences.append(text)\n",
    "                break\n",
    "        \n",
    "        generated_sequences.append(output_sequences)\n",
    "        \n",
    "    return generated_sequences\n",
    "\n",
    "# Test the function with a prompt\n",
    "prompt = \"Write a Python function that calculates the sum of two numbers\"\n",
    "generated_code = generate_code(prompt, max_length=200, min_length=30)\n",
    "print(generated_code[0][0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
