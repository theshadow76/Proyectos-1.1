{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 4, Loss: 6.238898873329163\n",
      "Step: 8, Loss: 4.646116673946381\n",
      "Step: 12, Loss: 4.915012359619141\n",
      "Step: 16, Loss: 4.767178654670715\n",
      "Step: 20, Loss: 4.714836835861206\n",
      "Step: 24, Loss: nan\n",
      "Step: 28, Loss: nan\n",
      "Step: 32, Loss: 4.84111225605011\n",
      "Step: 36, Loss: 4.614004850387573\n",
      "Step: 40, Loss: 4.24582040309906\n",
      "Step: 44, Loss: nan\n",
      "Step: 48, Loss: 4.8868491649627686\n",
      "Step: 52, Loss: 4.204628109931946\n",
      "Step: 56, Loss: 4.725884556770325\n",
      "Step: 60, Loss: nan\n",
      "Step: 64, Loss: 4.646574258804321\n",
      "Step: 68, Loss: 4.697717070579529\n",
      "Step: 72, Loss: 4.597574174404144\n",
      "Step: 76, Loss: 3.7510640621185303\n",
      "Step: 80, Loss: 4.9048765897750854\n",
      "Step: 84, Loss: 4.624587059020996\n",
      "Step: 88, Loss: nan\n",
      "Step: 92, Loss: 4.083964765071869\n",
      "Step: 96, Loss: 4.071129739284515\n",
      "Step: 100, Loss: 4.622526526451111\n",
      "Step: 104, Loss: 3.3688693940639496\n",
      "Step: 108, Loss: 5.453556656837463\n",
      "Step: 112, Loss: 3.9155210852622986\n",
      "Step: 116, Loss: 4.29439902305603\n",
      "Step: 120, Loss: 3.888221800327301\n",
      "Step: 124, Loss: nan\n",
      "Step: 128, Loss: nan\n",
      "Step: 132, Loss: 3.6632338166236877\n",
      "Step: 136, Loss: 3.4603028893470764\n",
      "Step: 140, Loss: 4.220409452915192\n",
      "Step: 144, Loss: 4.97872793674469\n",
      "Step: 148, Loss: nan\n",
      "Step: 152, Loss: 4.62292355298996\n",
      "Epoch: 1, Loss: 4.272590160369873\n",
      "Step: 4, Loss: 3.8521063327789307\n",
      "Step: 8, Loss: nan\n",
      "Step: 12, Loss: nan\n",
      "Step: 16, Loss: 3.791764497756958\n",
      "Step: 20, Loss: 3.317304015159607\n",
      "Step: 24, Loss: nan\n",
      "Step: 28, Loss: 3.775128424167633\n",
      "Step: 32, Loss: 3.477453112602234\n",
      "Step: 36, Loss: 3.859752118587494\n",
      "Step: 40, Loss: 5.198726177215576\n",
      "Step: 44, Loss: 3.537702977657318\n",
      "Step: 48, Loss: nan\n",
      "Step: 52, Loss: 4.347485661506653\n",
      "Step: 56, Loss: 3.516168773174286\n",
      "Step: 60, Loss: 4.172967433929443\n",
      "Step: 64, Loss: 3.5029256343841553\n",
      "Step: 68, Loss: 4.5459489822387695\n",
      "Step: 72, Loss: 3.43078875541687\n",
      "Step: 76, Loss: 3.521841585636139\n",
      "Step: 80, Loss: nan\n",
      "Step: 84, Loss: 3.8027592301368713\n",
      "Step: 88, Loss: 3.4968336820602417\n",
      "Step: 92, Loss: 4.522616624832153\n",
      "Step: 96, Loss: 3.424035131931305\n",
      "Step: 100, Loss: nan\n",
      "Step: 104, Loss: 3.0811502933502197\n",
      "Step: 108, Loss: 3.6353949904441833\n",
      "Step: 112, Loss: 3.8583258986473083\n",
      "Step: 116, Loss: 3.4879205226898193\n",
      "Step: 120, Loss: 3.783946454524994\n",
      "Step: 124, Loss: 3.4337780475616455\n",
      "Step: 128, Loss: 3.336003601551056\n",
      "Step: 132, Loss: nan\n",
      "Step: 136, Loss: 3.6897783279418945\n",
      "Step: 140, Loss: 4.08279824256897\n",
      "Step: 144, Loss: 3.070271372795105\n",
      "Step: 148, Loss: 4.027781367301941\n",
      "Step: 152, Loss: 3.673519551753998\n",
      "Epoch: 2, Loss: 4.408758640289307\n",
      "Step: 4, Loss: 3.068445384502411\n",
      "Step: 8, Loss: 4.3496173620224\n",
      "Step: 12, Loss: nan\n",
      "Step: 16, Loss: 3.05444598197937\n",
      "Step: 20, Loss: nan\n",
      "Step: 24, Loss: 4.724214196205139\n",
      "Step: 28, Loss: 3.300163745880127\n",
      "Step: 32, Loss: 3.652817130088806\n",
      "Step: 36, Loss: 4.02482682466507\n",
      "Step: 40, Loss: 2.9075657725334167\n",
      "Step: 44, Loss: 4.804947018623352\n",
      "Step: 48, Loss: nan\n",
      "Step: 52, Loss: 3.4488810300827026\n",
      "Step: 56, Loss: 3.2268722653388977\n",
      "Step: 60, Loss: 3.510109603404999\n",
      "Step: 64, Loss: 3.2786333560943604\n",
      "Step: 68, Loss: 3.1955636739730835\n",
      "Step: 72, Loss: 2.883302628993988\n",
      "Step: 76, Loss: nan\n",
      "Step: 80, Loss: 3.17462095618248\n",
      "Step: 84, Loss: 3.56456059217453\n",
      "Step: 88, Loss: nan\n",
      "Step: 92, Loss: nan\n",
      "Step: 96, Loss: 3.1343449354171753\n",
      "Step: 100, Loss: nan\n",
      "Step: 104, Loss: 4.444244861602783\n",
      "Step: 108, Loss: 2.963310658931732\n",
      "Step: 112, Loss: 2.7050832509994507\n",
      "Step: 116, Loss: nan\n",
      "Step: 120, Loss: 2.866515636444092\n",
      "Step: 124, Loss: nan\n",
      "Step: 128, Loss: 2.5096402764320374\n",
      "Step: 132, Loss: 4.032295346260071\n",
      "Step: 136, Loss: 3.1366947293281555\n",
      "Step: 140, Loss: 4.368348240852356\n",
      "Step: 144, Loss: 3.065165877342224\n",
      "Step: 148, Loss: 2.6800113916397095\n",
      "Step: 152, Loss: 3.0133867263793945\n",
      "Epoch: 3, Loss: 3.5274558067321777\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, DataCollatorForLanguageModeling\n",
    "\n",
    "class CodeDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=512):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data[idx]\n",
    "        tokens = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
    "        tokens = {key: value.squeeze(0) for key, value in tokens.items()}\n",
    "        return tokens\n",
    "\n",
    "# Load data from txt file\n",
    "with open('data6.txt', 'r') as f:\n",
    "    data = f.readlines()\n",
    "\n",
    "# Initialize the GPT-2 tokenizer and dataset\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "max_length = 128  # Reduced max_length\n",
    "dataset = CodeDataset(data, tokenizer, max_length=max_length)\n",
    "\n",
    "# Create a DataLoader for batching\n",
    "batch_size = 2  # Reduced batch_size\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=data_collator)\n",
    "\n",
    "# Load the pre-trained GPT-2 model and set it up for training\n",
    "model = GPT2LMHeadModel.from_pretrained('distilgpt2')  # Changed to 'distilgpt2'\n",
    "\n",
    "# Check if multiple GPUs are available and wrap the model with nn.DataParallel\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Using\", torch.cuda.device_count(), \"GPUs\")\n",
    "    model = torch.nn.DataParallel(model)\n",
    "\n",
    "model.train()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Set up the optimizer and training loop\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "epochs = 3\n",
    "gradient_accumulation_steps = 4  # Gradient accumulation\n",
    "accumulated_loss = 0.0\n",
    "for epoch in range(epochs):\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient accumulation\n",
    "        accumulated_loss += loss.item()\n",
    "        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            print(f\"Step: {step + 1}, Loss: {accumulated_loss / gradient_accumulation_steps}\")\n",
    "            accumulated_loss = 0.0\n",
    "\n",
    "    print(f\"Epoch: {epoch + 1}, Loss: {loss.item()}\")\n",
    "\n",
    "# Save the trained model\n",
    "model.save_pretrained(\"trained_7_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a variable that's called StrVigo and has a value of 'hi'\n",
      "\n",
      "You can start by using a method, such as:\n",
      "#!/bin/bash\n",
      "If you need to use more than one method like:<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Load the original GPT-2 tokenizer\n",
    "original_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Save the tokenizer files to the \"trained_gpt2_model\" folder\n",
    "original_tokenizer.save_pretrained(\"trained_3_model\")\n",
    "\n",
    "# Load the saved model\n",
    "model = GPT2LMHeadModel.from_pretrained(\"trained_3_model\")\n",
    "model.eval()\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"trained_3_model\")\n",
    "\n",
    "# Function to generate code based on a prompt\n",
    "def generate_code(prompt, max_length=None, num_return_sequences=3, min_length=50):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    attention_mask = torch.ones(input_ids.shape, dtype=torch.long, device=input_ids.device)\n",
    "    generated_sequences = []\n",
    "    \n",
    "    for i in range(num_return_sequences):\n",
    "        output_sequences = []\n",
    "        while True:\n",
    "            output_sequence = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=max_length,\n",
    "                no_repeat_ngram_size=2,\n",
    "                do_sample=True,\n",
    "                top_k=50, # You can experiment with this value\n",
    "                top_p=0.95,\n",
    "                temperature=1.0, # You can experiment with this value\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "            generated_sequence = output_sequence[0].tolist()\n",
    "            text = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)\n",
    "            \n",
    "            if len(text) >= min_length:\n",
    "                output_sequences.append(text)\n",
    "                break\n",
    "        \n",
    "        generated_sequences.append(output_sequences)\n",
    "        \n",
    "    return generated_sequences\n",
    "\n",
    "# Test the function with a prompt\n",
    "prompt = \"Write a variable that's called StrVigo and has a value of 'hi'\"\n",
    "generated_code = generate_code(prompt, max_length=200, min_length=30)\n",
    "print(generated_code[0][0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
