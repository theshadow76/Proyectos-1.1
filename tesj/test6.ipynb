{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, trainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_py_files(directory):\n",
    "    code_snippets = []\n",
    "\n",
    "    for file_name in os.listdir(directory):\n",
    "        if file_name.endswith('.py'):\n",
    "            file_path = os.path.join(directory, file_name)\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                content = file.read()\n",
    "                code_snippets.append(content)\n",
    "    \n",
    "    return code_snippets\n",
    "\n",
    "directory = 'C:\\\\Users\\\\vigop\\\\OneDrive\\\\backup 1\\\\coding\\\\data\\\\python-deeplearning\\\\train'\n",
    "code_snippets = read_py_files(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_comments(code):\n",
    "    # Remove single-line comments\n",
    "    code = re.sub(r'#.*', '', code)\n",
    "    \n",
    "    # Remove multi-line comments\n",
    "    code = re.sub(r\"'''[\\s\\S]*?'''\", '', code)\n",
    "    code = re.sub(r'\"\"\"[\\s\\S]*?\"\"\"', '', code)\n",
    "    \n",
    "    return code\n",
    "\n",
    "def remove_empty_lines(code):\n",
    "    lines = code.split('\\n')\n",
    "    non_empty_lines = [line.strip() for line in lines if line.strip()]\n",
    "    return '\\n'.join(non_empty_lines)\n",
    "\n",
    "def preprocess_code_snippets(code_snippets):\n",
    "    preprocessed_snippets = []\n",
    "    \n",
    "    for snippet in code_snippets:\n",
    "        # Remove comments\n",
    "        snippet = remove_comments(snippet)\n",
    "        \n",
    "        # Remove empty lines\n",
    "        snippet = remove_empty_lines(snippet)\n",
    "        \n",
    "        preprocessed_snippets.append(snippet)\n",
    "    \n",
    "    return preprocessed_snippets\n",
    "\n",
    "preprocessed_snippets = preprocess_code_snippets(code_snippets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the tokenizer\n",
    "def train_tokenizer(snippets, vocab_size=30000):\n",
    "    tokenizer = Tokenizer(models.BPE())\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "\n",
    "    trainer = trainers.BpeTrainer(vocab_size=vocab_size, special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])\n",
    "    tokenizer.train_from_iterator(snippets, trainer)\n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "vocab_size = 30000\n",
    "tokenizer = train_tokenizer(preprocessed_snippets, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super(Seq2SeqModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        output = self.fc(lstm_out)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the preprocessed snippets\n",
    "def tokenize_code_snippets(snippets, tokenizer):\n",
    "    tokenized_snippets = [tokenizer.encode(snippet) for snippet in snippets]\n",
    "    return tokenized_snippets\n",
    "\n",
    "# Use the trained tokenizer\n",
    "tokenized_snippets = tokenize_code_snippets(preprocessed_snippets, tokenizer)\n",
    "\n",
    "# Create a dataset and dataloader for the tokenized snippets\n",
    "class CodeDataset(Dataset):\n",
    "    def __init__(self, tokenized_snippets):\n",
    "        self.tokenized_snippets = tokenized_snippets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_snippets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.tokenized_snippets[idx]\n",
    "\n",
    "dataset = CodeDataset(tokenized_snippets)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "\n",
    "# Use the Seq2SeqModel class\n",
    "input_size = vocab_size\n",
    "hidden_size = 256\n",
    "output_size = vocab_size\n",
    "num_layers = 1\n",
    "\n",
    "model = Seq2SeqModel(input_size, hidden_size, output_size, num_layers)\n",
    "\n",
    "# Train and save the model\n",
    "def train_and_save_model(model, dataloader, num_epochs=10):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for i, data in enumerate(dataloader, 0):\n",
    "            inputs = data.to(device)\n",
    "            # Implement the logic for obtaining labels for your specific task\n",
    "            labels = ... # Update this line to obtain appropriate labels\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.view(-1, outputs.size(-1)), labels.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch {epoch + 1}, Loss: {running_loss / len(dataloader)}')\n",
    "\n",
    "    print(\"Finished training\")\n",
    "    torch.save(model.state_dict(), 'trained_model.pth')\n",
    "\n",
    "train_and_save_model(model, dataloader, num_epochs=10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
