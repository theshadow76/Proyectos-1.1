{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import os\n",
    "import gc\n",
    "import torch.cuda as cuda\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb=256'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. GPU: NVIDIA GeForce GTX 1650 SUPER\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for training and tokenizing\n",
    "def all_texts(data):\n",
    "    for item in data:\n",
    "        nl = item.get('nl', '')\n",
    "        code = item.get('code', '')\n",
    "        yield nl\n",
    "        yield code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tokenizer_on_concode_data(train_data, vocab_size=30000):\n",
    "    tokenizer = Tokenizer(models.BPE())\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=True)\n",
    "    tokenizer.decoder = decoders.ByteLevel()\n",
    "    trainer = trainers.BpeTrainer(vocab_size=vocab_size, special_tokens=[\"<s>\", \"<pad>\", \"</s>\", \"<unk>\"])\n",
    "    tokenizer.train_from_iterator(all_texts(train_data), trainer)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_concode_dataset(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            example = json.loads(line.strip())\n",
    "            data.append(example)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_concode_dataset(data, tokenizer):\n",
    "    tokenized_data = []\n",
    "    for item in data:\n",
    "        if 'code_tokens' in item and 'docstring_tokens' in item:\n",
    "            tokenized_code = tokenizer.encode(' '.join(item['code_tokens']))\n",
    "            tokenized_docstring = tokenizer.encode(' '.join(item['docstring_tokens']))\n",
    "            tokenized_data.append((tokenized_code, tokenized_docstring))\n",
    "    return tokenized_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_collate(batch):\n",
    "    def pad_sequences(sequences, max_len):\n",
    "        padded_seqs = torch.zeros(len(sequences), max_len, dtype=torch.long)\n",
    "        for i, seq in enumerate(sequences):\n",
    "            padded_seqs[i, :len(seq)] = seq\n",
    "        return padded_seqs\n",
    "\n",
    "    code_tokens = [item['code_tokens'] for item in batch]\n",
    "    nl_tokens = [item['nl_tokens'] for item in batch]\n",
    "\n",
    "    max_code_len = max([len(t) for t in code_tokens])\n",
    "    max_nl_len = max([len(t) for t in nl_tokens])\n",
    "\n",
    "    padded_code_tokens = pad_sequences(code_tokens, max_code_len)\n",
    "    padded_nl_tokens = pad_sequences(nl_tokens, max_nl_len)\n",
    "\n",
    "    return {'code_tokens': padded_code_tokens, 'nl_tokens': padded_nl_tokens}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcodeDataset(Dataset):\n",
    "    def __init__(self, tokenized_data):\n",
    "        self.tokenized_data = tokenized_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        item = self.tokenized_data[idx]\n",
    "        code_tokens, nl_tokens = item\n",
    "\n",
    "        # Convert tokenizers.Encoding objects to PyTorch tensors\n",
    "        code_tokens_tensor = torch.tensor(code_tokens.ids)\n",
    "        nl_tokens_tensor = torch.tensor(nl_tokens.ids)\n",
    "\n",
    "        sample = {'code_tokens': code_tokens_tensor, 'nl_tokens': nl_tokens_tensor}\n",
    "        return sample\n",
    "\n",
    "# Model classes\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        outputs, (hidden, cell) = self.lstm(x)\n",
    "        return hidden, cell\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_size, hidden_size, num_layers):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.output_dim = output_size\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden, cell, trg):  # Add trg as input argument\n",
    "        x = self.embedding(x)\n",
    "        outputs, (hidden, cell) = self.lstm(x, (hidden, cell))  # Pass hidden and cell states to LSTM\n",
    "        output = self.fc(outputs.squeeze(1))\n",
    "        return output, hidden, cell, trg  # Return trg as well\n",
    "\n",
    "\n",
    "class EncoderDecoderModel(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(EncoderDecoderModel, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        hidden, cell = self.encoder(src)  # Unpack hidden and cell here\n",
    "        batch_size = src.size(0)  # Add this line to get the batch size\n",
    "        decoder_input = torch.tensor([SOS_IDX] * batch_size, device=src.device).unsqueeze(1)  # Modify this line to match the batch size\n",
    "        decoder_outputs = torch.zeros(trg.shape[0], trg.shape[1], self.decoder.output_dim, device=src.device)\n",
    "\n",
    "        for t in range(trg.shape[1]):\n",
    "            output, hidden, cell = self.decoder(decoder_input, hidden, cell) # Use cell here\n",
    "            decoder_outputs[:, t] = output\n",
    "            decoder_input = trg[:, t].unsqueeze(1)\n",
    "\n",
    "        return decoder_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_model(model, dataloader, device, num_epochs=1, accumulation_steps=4):\n",
    "    model.to(device)\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(\"Using \", torch.cuda.device_count(), \" GPUs\")\n",
    "        model = nn.DataParallel(model)\n",
    "    try:\n",
    "        output = model(nl_tokens.to(device))  # Pass the nl_tokens tensor here\n",
    "    except RuntimeError as e:\n",
    "        if \"out of memory\" in str(e):\n",
    "            print(f\"Out of memory error occurred on device {device}. Trying another device...\")\n",
    "            for i in range(1, torch.cuda.device_count()):\n",
    "                try:\n",
    "                    print(f\"Trying device {i}...\")\n",
    "                    model.to(f\"cuda:{i}\")\n",
    "                    output = model(nl_tokens.to(f\"cuda:{i}\"))  # Pass the nl_tokens tensor here\n",
    "                    break\n",
    "                except RuntimeError as e:\n",
    "                    if \"out of memory\" in str(e):\n",
    "                        print(f\"Out of memory error occurred on device {i}. Trying next device...\")\n",
    "                    else:\n",
    "                        raise e\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    scaler = GradScaler()\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True)\n",
    "    print_loss_total = 0  # initialize print_loss_total to 0\n",
    "    loss_plot = []  # initialize loss_plot to an empty list\n",
    "    print_every = 1\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for i, data in enumerate(dataloader, 0):\n",
    "            nl_tokens = data['nl_tokens'].to(device)\n",
    "            code_tokens = data['code_tokens'].to(device)\n",
    "\n",
    "            try:\n",
    "                outputs = model(nl_tokens)\n",
    "                loss = criterion(outputs.reshape(-1, outputs.size(-1)), code_tokens[:, 1:].reshape(-1))\n",
    "            except RuntimeError as e:\n",
    "                print(f\"Caught CUDA error: {e}\")\n",
    "                # Print memory usage summary\n",
    "                print(cuda.memory_summary())\n",
    "                # Clear the GPU memory and garbage collect\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "                continue\n",
    "\n",
    "            with autocast():\n",
    "                outputs = model(nl_tokens)\n",
    "                loss = criterion(outputs.reshape(-1, outputs.size(-1)), code_tokens[:, 1:].reshape(-1))\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            if (i + 1) % accumulation_steps == 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if (i + 1) % print_every == 0:\n",
    "                print(f'Epoch {epoch + 1}, Iteration {i + 1}, Running Loss: {running_loss / (i + 1)}')\n",
    "\n",
    "        epoch_loss = running_loss / len(dataloader)\n",
    "        scheduler.step(epoch_loss)\n",
    "        print(f'Epoch {epoch + 1}, Loss: {epoch_loss}')\n",
    "\n",
    "    print(\"Finished training\")\n",
    "    torch.save(model.state_dict(), 'trained_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 22178 examples from ./data1/python_train_13.jsonl\n",
      "Trained tokenizer with 22178 examples\n",
      "Tokenized 22178 examples\n",
      "Loaded tokenizer\n",
      "Created dataset with 22178 examples\n",
      "22178\n",
      "Created dataloader with 5545 batches\n",
      "the len of the dataset is: 22178\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20392\\1797066501.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Caught CUDA error: {e}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20392\\1797066501.py\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"cuda\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"cpu\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"the len of the dataset is: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconcode_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m         \u001b[0mnl_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconcode_dataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# Get the input tokens from the first example in the dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m         \u001b[0mtrain_and_save_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnl_tokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnl_tokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccumulation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    train_file = \"./data1/python_train_13.jsonl\"\n",
    "    train_data = load_concode_dataset(train_file)\n",
    "    print(f\"Loaded {len(train_data)} examples from {train_file}\")\n",
    "\n",
    "    vocab_size = 30000\n",
    "    tokenizer = train_tokenizer_on_concode_data(train_data, vocab_size)\n",
    "    print(f\"Trained tokenizer with {len(train_data)} examples\")\n",
    "    tokenizer.save(\"concode_tokenizer3.json\")\n",
    "\n",
    "    tokenized_data = tokenize_concode_dataset(train_data, tokenizer)\n",
    "    print(f\"Tokenized {len(tokenized_data)} examples\")\n",
    "\n",
    "    tokenizer = Tokenizer.from_file(\"concode_tokenizer3.json\")\n",
    "    print(\"Loaded tokenizer\")\n",
    "\n",
    "    global SOS_IDX  # Declare SOS_IDX as a global variable\n",
    "    SOS_IDX = tokenizer.token_to_id('<s>')\n",
    "\n",
    "    dataset = ConcodeDataset(tokenized_data)\n",
    "    print(f\"Created dataset with {len(dataset)} examples\")\n",
    "    concode_dataset = ConcodeDataset(tokenized_data)\n",
    "    dataloader = DataLoader(concode_dataset, batch_size=4, shuffle=True, num_workers=0, collate_fn=pad_collate)\n",
    "\n",
    "    print(len(concode_dataset))\n",
    "\n",
    "\n",
    "    print(f\"Created dataloader with {len(dataloader)} batches\")\n",
    "\n",
    "    input_size = vocab_size\n",
    "    hidden_size = 64\n",
    "    output_size = vocab_size\n",
    "    num_layers = 1\n",
    "\n",
    "    encoder = Encoder(input_size, hidden_size, num_layers)\n",
    "    decoder = Decoder(output_size, hidden_size, num_layers)\n",
    "    model = EncoderDecoderModel(encoder, decoder)\n",
    "\n",
    "    try:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(\"the len of the dataset is: \" + str(len(concode_dataset)))\n",
    "        nl_tokens = concode_dataset[0][0]  # Get the input tokens from the first example in the dataset\n",
    "        train_and_save_model(model, dataloader, device, nl_tokens=nl_tokens, num_epochs=1, accumulation_steps=8)\n",
    "        for i in range(5):\n",
    "            input_tokens, output_tokens = concode_dataset[i]\n",
    "            print(f\"Example {i}: input length={len(input_tokens)}, output length={len(output_tokens)}\")\n",
    "\n",
    "    except torch.cuda.CudaError as e:\n",
    "        print(f\"Caught CUDA error: {e}\")\n",
    "        # Clear the GPU memory and garbage collect\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        # Print memory usage summary\n",
    "        print(torch.cuda.memory_summary())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        main()\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Caught CUDA error: {e}\")\n",
    "        # Clear the GPU memory and garbage collect\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        # Print memory usage summary\n",
    "        print(torch.cuda.memory_summary()) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
